{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfe7036d",
   "metadata": {},
   "source": [
    "学习链接：https://zhuanlan.zhihu.com/p/89503068"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19e6a2a",
   "metadata": {},
   "source": [
    "## 图嵌入（Graph Embedding）\n",
    "\n",
    "图嵌入是一种 **表示学习（Representation Learning）** 技术，也称为 **网络嵌入**、**图表示学习** 或 **网络表示学习**。其目标是将图结构中的信息映射为低维空间中的向量表示，以便于后续的机器学习任务。\n",
    "\n",
    "### 1. 目标\n",
    "\n",
    "图嵌入主要包括两种类型：\n",
    "\n",
    "节点嵌入（Node Embedding）：将图中的每个节点表示成低维、实值、稠密的向量。\n",
    "\n",
    "*图级嵌入（Graph-level Embedding）：将整个图表示成一个低维、实值、稠密的向量。\n",
    "\n",
    "### 2. 图嵌入的方法\n",
    "\n",
    "图嵌入的实现方法多种多样，主要包括以下几类：\n",
    "\n",
    "#### 2.1 矩阵分解（Matrix Factorization）\n",
    "\n",
    "该方法通过构建和分解反映图结构的矩阵，获得节点或图的向量表示。常用的矩阵包括：\n",
    "\n",
    "* 邻接矩阵（Adjacency Matrix）\n",
    "* 拉普拉斯矩阵（Laplacian Matrix）\n",
    "* 节点转移概率矩阵（Transition Probability Matrix）\n",
    "* 节点属性矩阵（Node Feature Matrix）\n",
    "\n",
    "不同的矩阵类型适用于不同的分解策略，例如特征值分解、奇异值分解（SVD）等。此类方法适合结构静态、规模较小的图。\n",
    "\n",
    "#### 2.2 DeepWalk\n",
    "\n",
    "DeepWalk 是一种基于随机游走和 Word2Vec 模型的图嵌入方法。其核心思想包括：\n",
    "\n",
    "* 利用随机游走在图中生成节点序列；\n",
    "* 将这些序列视为“句子”，将节点视为“单词”；\n",
    "* 使用 Word2Vec 对这些“句子”进行训练，获得节点的嵌入表示。\n",
    "\n",
    "#### 2.3 图神经网络（Graph Neural Network, GNN）\n",
    "\n",
    "图神经网络通过结合图结构与节点特征，并使用神经网络进行端到端训练，实现图嵌入学习。GNN 类方法可以同时处理节点分类、图分类、链接预测等多种任务。\n",
    "\n",
    "**常见的 GNN 模型包括**：\n",
    "\n",
    "* GCN（Graph Convolutional Network）\n",
    "* GAT（Graph Attention Network）\n",
    "* GraphSAGE\n",
    "* Graph Isomorphism Network (GIN)\n",
    "\n",
    "GNN 不仅可以生成节点嵌入，也可以通过全局聚合机制生成图级嵌入，是目前图嵌入研究的核心方向之一。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79fab9b",
   "metadata": {},
   "source": [
    "## 3. 图嵌入方法图谱（Method Taxonomy）\n",
    "\n",
    "图嵌入方法可以从图类型、训练方法和传播方式（Propagating Step）等多个维度进行分类。如下是主要的分类图谱结构：\n",
    "\n",
    "### 3.1 按图类型分类（Graph Types）\n",
    "\n",
    "* **静态图（Static Graph）**\n",
    "* **异构图（Heterogeneous Graph）**\n",
    "\n",
    "  * Graph Inspection\n",
    "  * HAN\n",
    "* **边属性图（Edge-attributed Graph）**\n",
    "\n",
    "  * G2S\n",
    "  * R-GCN\n",
    "* **动态图（Dynamic Graph）**\n",
    "\n",
    "  * DCRNN\n",
    "  * STGCN\n",
    "  * Structural-RNN\n",
    "  * ST-GCN\n",
    "* **分级图（Graded Graph）**\n",
    "\n",
    "  * DGP\n",
    "\n",
    "### 3.2 按训练方法分类（Training Methods）\n",
    "\n",
    "* **邻居采样（Neighborhood Sampling）**\n",
    "\n",
    "  * GraphSAGE\n",
    "  * FastGCN\n",
    "  * PinSage\n",
    "  * SSE\n",
    "  * Adaptive Sampling\n",
    "* **感受野控制（Receptive Field Control）**\n",
    "\n",
    "  * Control Variate\n",
    "* **数据增强（Data Augmentation）**\n",
    "\n",
    "  * Co-training\n",
    "  * Self-training\n",
    "* **无监督训练（Unsupervised Training）**\n",
    "\n",
    "  * GAE\n",
    "  * VGAE\n",
    "  * ARGA\n",
    "  * GCMC\n",
    "\n",
    "### 3.3 按传播步骤分类（Propagation Step）\n",
    "\n",
    "* **卷积式聚合器（Convolutional Aggregator）**\n",
    "\n",
    "  * Graph Convolutional Networks（GCN）\n",
    "\n",
    "    * Spectral 类方法：Spectral Network, ChebNet, GCN\n",
    "    * Spatial 类方法：DCNN, MoNet, GraphSAGE 等\n",
    "* **注意力式聚合器（Attention Aggregator）**\n",
    "\n",
    "  * Graph Attention Network（GAT）\n",
    "  * Gated Attention Network\n",
    "* **门控更新器（Gate Updater）**\n",
    "\n",
    "  * GRU\n",
    "  * LSTM\n",
    "* **跳跃连接（Skip Connection）**\n",
    "\n",
    "  * Jump Knowledge Network\n",
    "  * Highway GNN\n",
    "* **分层结构（Hierarchical GNN）**\n",
    "\n",
    "  * ECC\n",
    "  * DIFFPOOL\n",
    "* **图结构 LSTM（Graph LSTM）**\n",
    "\n",
    "  * Tree LSTM\n",
    "  * Graph LSTM\n",
    "  * Sentence LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cfdcfe",
   "metadata": {},
   "source": [
    "## 4.图\n",
    "\n",
    "### 在图神经网络和图嵌入中基本定义：\n",
    "\n",
    "一个图记作：\n",
    "\n",
    "$$\n",
    "G = (V, E)\n",
    "$$\n",
    "\n",
    "其中：\n",
    "\n",
    "* $V$：节点的集合（Vertices）\n",
    "* $E$：边的集合（Edges），表示节点之间的连接关系\n",
    "\n",
    "### 节点特征表示：\n",
    "\n",
    "对于图中的每个节点 $i$，我们假设它拥有一个特征向量 $x_i$。将所有节点的特征组合在一起，可以构成一个特征矩阵：\n",
    "\n",
    "$$\n",
    "X \\in \\mathbb{R}^{N \\times D}\n",
    "$$\n",
    "\n",
    "其中：\n",
    "\n",
    "* $N$：节点数量（即 $|V|$）\n",
    "* $D$：每个节点的特征维度（即每个 $x_i \\in \\mathbb{R}^D$）\n",
    "\n",
    "这个矩阵 $X$ 就是图中所有节点的特征表示。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e6ecef",
   "metadata": {},
   "source": [
    "##  图的三种基本矩阵解释\n",
    "\n",
    "### 1. 邻接矩阵（Adjacency Matrix, $A$）\n",
    "\n",
    "邻接矩阵用来表示节点之间是否有**边相连**。\n",
    "\n",
    "* 是一个 $N \\times N$ 的矩阵（$N$ 是节点数）\n",
    "* 如果节点 $i$ 和节点 $j$ 有边相连，$A_{ij} = 1$，否则为 0。\n",
    "* 对于**无向图**，邻接矩阵是对称的。\n",
    "\n",
    "在图中可以看到：\n",
    "\n",
    "```\n",
    "    A =\n",
    "    0 1 1 0 0 0\n",
    "    1 0 1 1 0 0\n",
    "    1 1 0 0 1 0\n",
    "    0 1 0 0 1 1\n",
    "    0 0 1 1 0 0\n",
    "    0 0 0 1 0 0\n",
    "```\n",
    "\n",
    "每一行/列表示一个节点，相邻节点之间的值为 1。\n",
    "\n",
    "---\n",
    "\n",
    "### 2. 度矩阵（Degree Matrix, $D$）\n",
    "\n",
    "度矩阵是一个对角矩阵，用来表示每个节点的“度”——也就是它连接了几个边（邻居节点数）。\n",
    "\n",
    "* 是一个 $N \\times N$ 的**对角矩阵**\n",
    "* 第 $i$ 行第 $i$ 列的值 $D_{ii}$ 表示节点 $i$ 的度（它有多少邻居）\n",
    "\n",
    "在图中可以看到：\n",
    "\n",
    "```\n",
    "    D =\n",
    "    2 0 0 0 0 0\n",
    "    0 3 0 0 0 0\n",
    "    0 0 3 0 0 0\n",
    "    0 0 0 3 0 0\n",
    "    0 0 0 0 2 0\n",
    "    0 0 0 0 0 1\n",
    "```\n",
    "\n",
    "例如节点 2 有 3 个邻居，所以它的度为 3。\n",
    "\n",
    "---\n",
    "\n",
    "### 3. 拉普拉斯矩阵（Laplacian Matrix, $L$）\n",
    "\n",
    "拉普拉斯矩阵是最常用于图信号处理和图神经网络中的结构，它定义为：\n",
    "\n",
    "$$\n",
    "L = D - A\n",
    "$$\n",
    "\n",
    "即：**度矩阵 - 邻接矩阵**\n",
    "\n",
    "* 它综合反映了图的结构和连接方式\n",
    "* 常用于图卷积、频谱分析等领域\n",
    "\n",
    "在图中可以看到：\n",
    "\n",
    "```\n",
    "    L =\n",
    "    2 -1 -1  0  0  0\n",
    "   -1  3 -1 -1  0  0\n",
    "   -1 -1  3  0 -1  0\n",
    "    0 -1  0  3 -1 -1\n",
    "    0  0 -1 -1  2  0\n",
    "    0  0  0 -1  0  1\n",
    "```\n",
    "\n",
    "对角线元素是度，非对角线是邻接关系的负值。\n",
    "\n",
    "---\n",
    "\n",
    "## 总结\n",
    "\n",
    "| 矩阵                 | 表示意义          |\n",
    "| ------------------ | ------------- |\n",
    "| 邻接矩阵 $A$           | 哪些节点之间有边      |\n",
    "| 度矩阵 $D$            | 每个节点的连接数（度）   |\n",
    "| 拉普拉斯矩阵 $L = D - A$ | 图的结构，用于图卷积等操作 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c33c31d",
   "metadata": {},
   "source": [
    "## 5. 图卷积的公式理解\n",
    "\n",
    "GCN的核心在于如何基于图结构 进行特征的聚合与传播。\n",
    "\n",
    "### 总体框架：图卷积的一般形式\n",
    "\n",
    "$$\n",
    "H^{(l+1)} = f(H^{(l)}, A)\n",
    "$$\n",
    "\n",
    "常见的简化版本为：\n",
    "\n",
    "$$\n",
    "H^{(l+1)} = \\sigma(AH^{(l)}W^{(l)})\n",
    "$$\n",
    "\n",
    "其中：\n",
    "\n",
    "* $H^{(l)}$：第 $l$ 层节点的特征表示\n",
    "* $A$：邻接矩阵\n",
    "* $W^{(l)}$：第 $l$ 层的可训练权重矩阵\n",
    "* $\\sigma$：非线性激活函数（如 ReLU）\n",
    "\n",
    "\n",
    "### 实现一：基础图卷积（未归一化）\n",
    "\n",
    "$$\n",
    "H^{(l+1)} = \\sigma(AH^{(l)}W^{(l)})\n",
    "$$\n",
    "\n",
    "**特点：**\n",
    "\n",
    "* 聚合邻居特征，但没有考虑自身特征；\n",
    "* 邻接矩阵 $A$ 未归一化，容易造成高阶节点影响过大。\n",
    "\n",
    "类比：只听邻居的，不考虑自己的信息，且说话声音大小没有控制。\n",
    "\n",
    "\n",
    "### 实现二：引入拉普拉斯矩阵（考虑自身）\n",
    "\n",
    "$$\n",
    "H^{(l+1)} = \\sigma((D - A)H^{(l)}W^{(l)}) = \\sigma(L H^{(l)}W^{(l)})\n",
    "$$\n",
    "\n",
    "**特点：**\n",
    "\n",
    "* 使用组合拉普拉斯矩阵 $L = D - A$，引入自身节点的影响；\n",
    "* 缓解了特征传递的偏差问题。\n",
    "\n",
    "类比：开始考虑自己的声音，但邻居声音仍未做“降噪”处理。\n",
    "\n",
    "\n",
    "### 实现三：标准 GCN（对称归一化）\n",
    "\n",
    "$$\n",
    "H^{(l+1)} = \\sigma(\\hat{D}^{-1/2} \\hat{A} \\hat{D}^{-1/2} H^{(l)} W^{(l)})\n",
    "$$\n",
    "\n",
    "其中：\n",
    "\n",
    "* $\\hat{A} = A + I$：邻接矩阵加上自环\n",
    "* $\\hat{D}$：对应的度矩阵\n",
    "\n",
    "\n",
    "$\\hat{D}^{-1/2}$ 例子：\n",
    "\n",
    "$$\n",
    "d_1 = 3, \\quad d_2 = 2, \\quad d_3 = 1\n",
    "$$\n",
    "\n",
    "那么度矩阵：\n",
    "\n",
    "$$\n",
    "\\hat{D} =\n",
    "\\begin{bmatrix}\n",
    "3 & 0 & 0 \\\\\n",
    "0 & 2 & 0 \\\\\n",
    "0 & 0 & 1 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "则：\n",
    "\n",
    "$$\n",
    "\\hat{D}^{-1/2} =\n",
    "\\begin{bmatrix}\n",
    "\\frac{1}{\\sqrt{3}} & 0 & 0 \\\\\n",
    "0 & \\frac{1}{\\sqrt{2}} & 0 \\\\\n",
    "0 & 0 & 1 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**特点：**\n",
    "\n",
    "* 自身信息 + 邻居信息；\n",
    "* 一次性更新所有节点\n",
    "* 邻接矩阵归一化，控制不同度节点的权重影响；\n",
    "* 两边乘 可以使结果对称且保持节点间影响平衡\n",
    "* 是 **Kipf & Welling (2017)** 提出的主流 GCN 实现方式。\n",
    "\n",
    "类比：你和邻居都发声，而且大家的音量都控制在相似水平，不让“话痨”主导讨论。\n",
    "\n",
    "### 从节点角度看更新公式（局部视角）\n",
    "\n",
    "$$\n",
    "h_i^{(l+1)} = \\sigma\\left(\\sum_{j \\in \\mathcal{N}(i) \\cup \\{i\\}} \\frac{1}{\\sqrt{d_i d_j}} h_j^{(l)} W^{(l)}\\right)\n",
    "$$\n",
    "\n",
    "\n",
    "where：\n",
    "* $h_j^{(l)}$：是**节点 $j$** 在第 $l$ 层的特征向量\n",
    "* $d_i$：是节点 $i$ 的度（连接多少节点，**包含自己**，因为 $\\hat{A} = A + I$）\n",
    "* $\\frac{1}{\\sqrt{d_i d_j}}$：归一化因子，避免高/低度节点影响失衡。\n",
    "\n",
    "\n",
    "### 实现方式对比总结：\n",
    "\n",
    "| 实现编号 | 公式形式                                               | 优点         | 缺点         |\n",
    "| ---- | -------------------------------------------------- | ---------- | ---------- |\n",
    "| 实现一  | $\\sigma(AHW)$                                      | 简洁直接       | 忽略自身，未归一化  |\n",
    "| 实现二  | $\\sigma(LHW)$                                      | 引入自环       | 无归一化，表达力弱  |\n",
    "| 实现三  | $\\sigma(\\hat{D}^{-1/2} \\hat{A} \\hat{D}^{-1/2} HW)$ | 平衡归一化、考虑自身 | 主流方案，结构更复杂 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d11b77c",
   "metadata": {},
   "source": [
    "## GCN代码构建"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c735f9b",
   "metadata": {},
   "source": [
    "GCN Layer \n",
    "\n",
    "$$\n",
    "H^{(l+1)} = \\sigma(\\hat{D}^{-1/2} \\hat{A} \\hat{D}^{-1/2} H^{(l)} W^{(l)})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffa5d5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入规范： 标准库 > 第三方库 > 自定义模块，每组之间空一行\n",
    "\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "\n",
    "class GraphConvolution(nn.Module):                  # 继承自 PyTorch 的 nn.Module 基类\n",
    "    def __init__(self, input, output, bias=True):\n",
    "        #super(GraphConvolution, self).__init__()   # 父类的初始化，确保具备功能.parameters()、.cuda()、.forward() 等。\n",
    "        super().__init__()                          # 简化版本\n",
    "        self.in_features = input\n",
    "        self.out_features = output\n",
    "\n",
    "        # 可训练参数：权重和偏置\n",
    "        self.weight = Parameter(torch.FloatTensor(input, output))    # 创建大小为 (input, output) 的权重矩阵，可优化参数\n",
    "        if bias:                                                     # torch.FloatTensor(...) 先创建一个未初始化的浮点张量。\n",
    "            self.bias = Parameter(torch.FloatTensor(output))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        self.reset_parameters()                                      # 初始化权重和偏置的数值\n",
    "\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"\n",
    "        初始化权重矩阵和偏置项。\n",
    "\n",
    "        权重和偏置值在 [-1/sqrt(output_dim), 1/sqrt(output_dim)] 范围内均匀采样，\n",
    "        以确保初始参数不会过大或过小，利于模型训练稳定性。\n",
    "        \"\"\"\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))                   # 找到初始化边界  [-1/√output_dim, 1/√output_dim] \n",
    "        self.weight.data.uniform_(-stdv, stdv)                       # 对权重矩阵采样，均匀分布，区间：[-1/√output_dim, 1/√output_dim]\n",
    "        if self.bias is not None:                                    # 存在bais 也同样方式采样\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "\n",
    "    def forward(self, input, adjacent):\n",
    "        '''\n",
    "        这里传入的adjacent 已经是归一化过的\\hat{D}^{-1/2} \\hat{A} \\hat{D}^{-1/2} \n",
    "        '''\n",
    "        support = torch.mm(input, self.weight)                      # XW \n",
    "        output = torch.spmm(adjacent, support)                      # AXW \n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "692211ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the GCN model structure\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    \"\"\"\n",
    "    基于两层图卷积(GCN)的图神经网络模型, 用于节点分类任务。\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout):\n",
    "        \"\"\"\n",
    "        初始化 GCN 模型结构。\n",
    "\n",
    "        参数:\n",
    "            nfeat (int): 输入特征的维度(每个节点的原始特征维度)\n",
    "            nhid (int): 第一层图卷积的输出维度(隐藏层大小)\n",
    "            nclass (int): 输出的类别数量(最终分类的类别数)\n",
    "            dropout (float): Dropout 概率(防止过拟合)\n",
    "        \"\"\"\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        self.gc1 = GraphConvolution(nfeat, nhid)     # 第一层图卷积, 从输入特征维度映射到隐藏维度\n",
    "        self.gc2 = GraphConvolution(nhid, nclass)    # 第二层图卷积, 从隐藏层输出到类别维度\n",
    "        self.dropout = dropout                        # Dropout 概率\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        \"\"\"\n",
    "        定义 GCN 模型的前向传播逻辑。\n",
    "\n",
    "        参数:\n",
    "            x (Tensor): 输入的节点特征矩阵, 形状为 [N, nfeat]\n",
    "            adj (Tensor): 归一化邻接矩阵(稀疏矩阵), 形状为 [N, N]\n",
    "\n",
    "        返回:\n",
    "            Tensor: 每个节点在每个类别上的对数概率, 形状为 [N, nclass]\n",
    "        \"\"\"\n",
    "        x = F.relu(self.gc1(x, adj))                             # 第一层图卷积 + ReLU 激活\n",
    "        x = F.dropout(x, self.dropout, training=self.training)  # Dropout, 用于训练阶段防止过拟合\n",
    "        x = self.gc2(x, adj)                                     # 第二层图卷积, 不加激活函数\n",
    "        return F.log_softmax(x, dim=1)                           # 对每个节点输出做 log_softmax, 用于分类\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3ad4f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "\n",
    "def encode_onehot(labels):\n",
    "    \"\"\"将标签转换为 one-hot 编码\"\"\"\n",
    "    classes = sorted(set(labels))\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in enumerate(classes)}\n",
    "    return np.array([classes_dict[label] for label in labels], dtype=np.int32)\n",
    "\n",
    "def normalize(mx):\n",
    "    \"\"\"行归一化: D^-1 A\"\"\"\n",
    "    rowsum = np.array(mx.sum(1)).flatten()\n",
    "    r_inv = np.power(rowsum, -1, where=rowsum != 0)\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    return r_mat_inv.dot(mx)\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"将 scipy 稀疏矩阵转换为 PyTorch 稀疏张量\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse_coo_tensor(indices, values, shape)\n",
    "\n",
    "def load_cora_data(path=\"./dataset/cora/\", dataset=\"cora\"):\n",
    "    print(f\"Loading {dataset} dataset...\")\n",
    "\n",
    "    # 读取 cora.content\n",
    "    content_file = f\"{path}{dataset}.content\"\n",
    "    idx_features_labels = np.genfromtxt(content_file, dtype=str)\n",
    "    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n",
    "    labels = encode_onehot(idx_features_labels[:, -1])\n",
    "\n",
    "    # 建立节点 ID 到索引的映射\n",
    "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
    "    idx_map = {j: i for i, j in enumerate(idx)}\n",
    "\n",
    "    # 读取 cora.cites 并建立边\n",
    "    cites_file = f\"{path}{dataset}.cites\"\n",
    "    edges_unordered = np.genfromtxt(cites_file, dtype=np.int32)\n",
    "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n",
    "                     dtype=np.int32).reshape(edges_unordered.shape)\n",
    "\n",
    "    # 构建邻接矩阵（非对称）\n",
    "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
    "                        shape=(labels.shape[0], labels.shape[0]), dtype=np.float32)\n",
    "\n",
    "    # 构建对称邻接矩阵 A + A^T\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "\n",
    "    # 邻接矩阵归一化 + 自环 A_hat = D^-1/2 (A + I) D^-1/2\n",
    "    adj = normalize(adj + sp.eye(adj.shape[0]))\n",
    "\n",
    "    # 数据类型转换为 PyTorch Tensor\n",
    "    features = torch.FloatTensor(np.array(features.todense()))\n",
    "    labels = torch.LongTensor(np.where(labels)[1])\n",
    "    adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "\n",
    "    # 划分数据集索引\n",
    "    idx_train = torch.LongTensor(range(140))\n",
    "    idx_val = torch.LongTensor(range(200, 500))\n",
    "    idx_test = torch.LongTensor(range(500, 1500))\n",
    "\n",
    "    return adj, features, labels, idx_train, idx_val, idx_test\n",
    "\n",
    "def accuracy(output, labels):\n",
    "    preds = output.max(1)[1]           # 每一行最大值所在的下标，就是预测的类别\n",
    "    correct = preds.eq(labels).sum()   # 和真实标签比对，计算匹配的个数\n",
    "    return correct.item() / len(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbdf8ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前工作路径是： e:\\code\\-\\DGNN以及ODE学习\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "cwd = os.getcwd()\n",
    "print(\"当前工作路径是：\", cwd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02dea284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n"
     ]
    }
   ],
   "source": [
    "# 调用，读取数据集\n",
    "adj, features, labels, idx_train, idx_val, idx_test = load_cora_data(\"./dataset/cora/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d515a464",
   "metadata": {},
   "source": [
    "## cora数据集介绍\n",
    "\n",
    "该数据集共2708个样本点，每个样本点都是一篇科学论文，所有样本点被分为8个类别，类别分别是1）基于案例；2）遗传算法；3）神经网络；4）概率方法；5）强化学习；6）规则学习；7）理论\n",
    "\n",
    "篇论文都由一个1433维的词向量表示，所以，每个样本点具有1433个特征\n",
    "\n",
    "下载的压缩包中有三个文件，分别是cora.cites，cora.content，README\n",
    "\n",
    "README是对数据集的介绍；cora.content是所有论文的独自的信息；cora.cites是论文之间的引用记录。\n",
    "\n",
    "cora.content共有2708行，每一行代表一个样本点，即一篇论文。\n",
    "\n",
    "ora.cites共5429行， 每一行有两个论文编号，表示第一个编号的论文先写，第二个编号的论文引用第一个编号的论文。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "967d62e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2708, 1433])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75561305",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.max().item() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "121c520c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001, Loss: 2.0955, Acc: 0.0714\n",
      "Epoch 002, Loss: 1.9694, Acc: 0.0714\n",
      "Epoch 003, Loss: 1.8602, Acc: 0.2643\n",
      "Epoch 004, Loss: 1.7582, Acc: 0.5214\n",
      "Epoch 005, Loss: 1.6945, Acc: 0.5643\n",
      "Epoch 006, Loss: 1.6274, Acc: 0.5929\n",
      "Epoch 007, Loss: 1.5501, Acc: 0.6214\n",
      "Epoch 008, Loss: 1.4766, Acc: 0.6214\n",
      "Epoch 009, Loss: 1.4012, Acc: 0.7143\n",
      "Epoch 010, Loss: 1.3319, Acc: 0.6643\n",
      "Epoch 011, Loss: 1.2120, Acc: 0.7286\n",
      "Epoch 012, Loss: 1.1488, Acc: 0.7286\n",
      "Epoch 013, Loss: 1.0570, Acc: 0.7643\n",
      "Epoch 014, Loss: 0.9234, Acc: 0.8143\n",
      "Epoch 015, Loss: 0.8975, Acc: 0.8357\n",
      "Epoch 016, Loss: 0.8160, Acc: 0.8286\n",
      "Epoch 017, Loss: 0.7533, Acc: 0.8714\n",
      "Epoch 018, Loss: 0.6532, Acc: 0.8571\n",
      "Epoch 019, Loss: 0.5895, Acc: 0.9071\n",
      "Epoch 020, Loss: 0.5629, Acc: 0.9000\n",
      "Epoch 021, Loss: 0.5152, Acc: 0.9214\n",
      "Epoch 022, Loss: 0.4978, Acc: 0.9286\n",
      "Epoch 023, Loss: 0.3944, Acc: 0.9429\n",
      "Epoch 024, Loss: 0.3340, Acc: 0.9429\n",
      "Epoch 025, Loss: 0.3610, Acc: 0.9643\n",
      "Epoch 026, Loss: 0.3122, Acc: 0.9286\n",
      "Epoch 027, Loss: 0.2922, Acc: 0.9500\n",
      "Epoch 028, Loss: 0.2524, Acc: 0.9643\n",
      "Epoch 029, Loss: 0.2102, Acc: 0.9571\n",
      "Epoch 030, Loss: 0.2921, Acc: 0.9286\n",
      "Epoch 031, Loss: 0.2281, Acc: 0.9429\n",
      "Epoch 032, Loss: 0.1943, Acc: 0.9429\n",
      "Epoch 033, Loss: 0.1953, Acc: 0.9643\n",
      "Epoch 034, Loss: 0.1870, Acc: 0.9500\n",
      "Epoch 035, Loss: 0.1569, Acc: 0.9643\n",
      "Epoch 036, Loss: 0.1489, Acc: 0.9714\n",
      "Epoch 037, Loss: 0.1518, Acc: 0.9714\n",
      "Epoch 038, Loss: 0.1379, Acc: 0.9643\n",
      "Epoch 039, Loss: 0.1300, Acc: 0.9714\n",
      "Epoch 040, Loss: 0.1194, Acc: 0.9786\n",
      "Epoch 041, Loss: 0.1665, Acc: 0.9500\n",
      "Epoch 042, Loss: 0.1163, Acc: 0.9714\n",
      "Epoch 043, Loss: 0.1262, Acc: 0.9571\n",
      "Epoch 044, Loss: 0.0862, Acc: 0.9929\n",
      "Epoch 045, Loss: 0.0854, Acc: 0.9857\n",
      "Epoch 046, Loss: 0.0982, Acc: 0.9857\n",
      "Epoch 047, Loss: 0.1075, Acc: 0.9786\n",
      "Epoch 048, Loss: 0.0729, Acc: 0.9857\n",
      "Epoch 049, Loss: 0.0864, Acc: 0.9857\n",
      "Epoch 050, Loss: 0.0828, Acc: 0.9786\n",
      "Saved model to pth/gcn_epoch50.pth\n",
      "Epoch 051, Loss: 0.1024, Acc: 0.9857\n",
      "Epoch 052, Loss: 0.0789, Acc: 0.9929\n",
      "Epoch 053, Loss: 0.0663, Acc: 0.9929\n",
      "Epoch 054, Loss: 0.0895, Acc: 0.9857\n",
      "Epoch 055, Loss: 0.0739, Acc: 0.9857\n",
      "Epoch 056, Loss: 0.0854, Acc: 0.9786\n",
      "Epoch 057, Loss: 0.0916, Acc: 0.9857\n",
      "Epoch 058, Loss: 0.0674, Acc: 0.9929\n",
      "Epoch 059, Loss: 0.0677, Acc: 0.9929\n",
      "Epoch 060, Loss: 0.0902, Acc: 0.9714\n",
      "Epoch 061, Loss: 0.0929, Acc: 0.9714\n",
      "Epoch 062, Loss: 0.0861, Acc: 0.9929\n",
      "Epoch 063, Loss: 0.0659, Acc: 0.9929\n",
      "Epoch 064, Loss: 0.1158, Acc: 0.9643\n",
      "Epoch 065, Loss: 0.0575, Acc: 0.9929\n",
      "Epoch 066, Loss: 0.0608, Acc: 0.9929\n",
      "Epoch 067, Loss: 0.0595, Acc: 1.0000\n",
      "Epoch 068, Loss: 0.0729, Acc: 0.9786\n",
      "Epoch 069, Loss: 0.0699, Acc: 0.9857\n",
      "Epoch 070, Loss: 0.0688, Acc: 0.9929\n",
      "Epoch 071, Loss: 0.0866, Acc: 0.9714\n",
      "Epoch 072, Loss: 0.0612, Acc: 0.9929\n",
      "Epoch 073, Loss: 0.0502, Acc: 0.9929\n",
      "Epoch 074, Loss: 0.0666, Acc: 0.9929\n",
      "Epoch 075, Loss: 0.0736, Acc: 0.9929\n",
      "Epoch 076, Loss: 0.0508, Acc: 1.0000\n",
      "Epoch 077, Loss: 0.0578, Acc: 0.9929\n",
      "Epoch 078, Loss: 0.0555, Acc: 0.9857\n",
      "Epoch 079, Loss: 0.0643, Acc: 0.9857\n",
      "Epoch 080, Loss: 0.0621, Acc: 0.9857\n",
      "Epoch 081, Loss: 0.0578, Acc: 0.9929\n",
      "Epoch 082, Loss: 0.0468, Acc: 1.0000\n",
      "Epoch 083, Loss: 0.0475, Acc: 0.9929\n",
      "Epoch 084, Loss: 0.0750, Acc: 0.9857\n",
      "Epoch 085, Loss: 0.0575, Acc: 0.9857\n",
      "Epoch 086, Loss: 0.0570, Acc: 1.0000\n",
      "Epoch 087, Loss: 0.0564, Acc: 0.9929\n",
      "Epoch 088, Loss: 0.0447, Acc: 1.0000\n",
      "Epoch 089, Loss: 0.0549, Acc: 0.9929\n",
      "Epoch 090, Loss: 0.0645, Acc: 1.0000\n",
      "Epoch 091, Loss: 0.0582, Acc: 0.9786\n",
      "Epoch 092, Loss: 0.0628, Acc: 0.9929\n",
      "Epoch 093, Loss: 0.0650, Acc: 0.9929\n",
      "Epoch 094, Loss: 0.0608, Acc: 0.9929\n",
      "Epoch 095, Loss: 0.0638, Acc: 0.9857\n",
      "Epoch 096, Loss: 0.0451, Acc: 1.0000\n",
      "Epoch 097, Loss: 0.0539, Acc: 1.0000\n",
      "Epoch 098, Loss: 0.0645, Acc: 0.9929\n",
      "Epoch 099, Loss: 0.0432, Acc: 0.9857\n",
      "Epoch 100, Loss: 0.0519, Acc: 1.0000\n",
      "Saved model to pth/gcn_epoch100.pth\n",
      "Epoch 101, Loss: 0.0589, Acc: 0.9929\n",
      "Epoch 102, Loss: 0.0545, Acc: 0.9929\n",
      "Epoch 103, Loss: 0.0733, Acc: 0.9714\n",
      "Epoch 104, Loss: 0.0526, Acc: 0.9857\n",
      "Epoch 105, Loss: 0.0474, Acc: 0.9929\n",
      "Epoch 106, Loss: 0.0393, Acc: 1.0000\n",
      "Epoch 107, Loss: 0.0319, Acc: 1.0000\n",
      "Epoch 108, Loss: 0.0502, Acc: 1.0000\n",
      "Epoch 109, Loss: 0.0529, Acc: 1.0000\n",
      "Epoch 110, Loss: 0.0456, Acc: 0.9929\n",
      "Epoch 111, Loss: 0.0681, Acc: 0.9857\n",
      "Epoch 112, Loss: 0.0651, Acc: 0.9786\n",
      "Epoch 113, Loss: 0.0300, Acc: 1.0000\n",
      "Epoch 114, Loss: 0.0344, Acc: 1.0000\n",
      "Epoch 115, Loss: 0.0392, Acc: 1.0000\n",
      "Epoch 116, Loss: 0.0517, Acc: 0.9929\n",
      "Epoch 117, Loss: 0.0433, Acc: 1.0000\n",
      "Epoch 118, Loss: 0.0451, Acc: 1.0000\n",
      "Epoch 119, Loss: 0.0438, Acc: 0.9929\n",
      "Epoch 120, Loss: 0.0438, Acc: 1.0000\n",
      "Epoch 121, Loss: 0.0482, Acc: 1.0000\n",
      "Epoch 122, Loss: 0.0447, Acc: 0.9929\n",
      "Epoch 123, Loss: 0.0487, Acc: 1.0000\n",
      "Epoch 124, Loss: 0.0512, Acc: 0.9929\n",
      "Epoch 125, Loss: 0.0486, Acc: 1.0000\n",
      "Epoch 126, Loss: 0.0542, Acc: 0.9929\n",
      "Epoch 127, Loss: 0.0446, Acc: 0.9857\n",
      "Epoch 128, Loss: 0.0488, Acc: 0.9929\n",
      "Epoch 129, Loss: 0.0483, Acc: 0.9929\n",
      "Epoch 130, Loss: 0.0541, Acc: 0.9929\n",
      "Epoch 131, Loss: 0.0366, Acc: 1.0000\n",
      "Epoch 132, Loss: 0.0555, Acc: 1.0000\n",
      "Epoch 133, Loss: 0.0544, Acc: 0.9929\n",
      "Epoch 134, Loss: 0.0414, Acc: 1.0000\n",
      "Epoch 135, Loss: 0.0406, Acc: 1.0000\n",
      "Epoch 136, Loss: 0.0349, Acc: 1.0000\n",
      "Epoch 137, Loss: 0.0543, Acc: 1.0000\n",
      "Epoch 138, Loss: 0.0460, Acc: 0.9929\n",
      "Epoch 139, Loss: 0.0329, Acc: 0.9929\n",
      "Epoch 140, Loss: 0.0436, Acc: 1.0000\n",
      "Epoch 141, Loss: 0.0404, Acc: 1.0000\n",
      "Epoch 142, Loss: 0.0517, Acc: 0.9857\n",
      "Epoch 143, Loss: 0.0340, Acc: 1.0000\n",
      "Epoch 144, Loss: 0.0314, Acc: 1.0000\n",
      "Epoch 145, Loss: 0.0310, Acc: 1.0000\n",
      "Epoch 146, Loss: 0.0405, Acc: 1.0000\n",
      "Epoch 147, Loss: 0.0582, Acc: 0.9857\n",
      "Epoch 148, Loss: 0.0603, Acc: 0.9857\n",
      "Epoch 149, Loss: 0.0349, Acc: 0.9929\n",
      "Epoch 150, Loss: 0.0339, Acc: 1.0000\n",
      "Saved model to pth/gcn_epoch150.pth\n",
      "Epoch 151, Loss: 0.0398, Acc: 0.9929\n",
      "Epoch 152, Loss: 0.0350, Acc: 1.0000\n",
      "Epoch 153, Loss: 0.0335, Acc: 0.9929\n",
      "Epoch 154, Loss: 0.0422, Acc: 0.9929\n",
      "Epoch 155, Loss: 0.0474, Acc: 0.9929\n",
      "Epoch 156, Loss: 0.0460, Acc: 1.0000\n",
      "Epoch 157, Loss: 0.0379, Acc: 1.0000\n",
      "Epoch 158, Loss: 0.0422, Acc: 0.9929\n",
      "Epoch 159, Loss: 0.0372, Acc: 0.9929\n",
      "Epoch 160, Loss: 0.0248, Acc: 1.0000\n",
      "Epoch 161, Loss: 0.0299, Acc: 1.0000\n",
      "Epoch 162, Loss: 0.0492, Acc: 0.9929\n",
      "Epoch 163, Loss: 0.0597, Acc: 0.9857\n",
      "Epoch 164, Loss: 0.0458, Acc: 0.9857\n",
      "Epoch 165, Loss: 0.0316, Acc: 1.0000\n",
      "Epoch 166, Loss: 0.0292, Acc: 1.0000\n",
      "Epoch 167, Loss: 0.0392, Acc: 0.9929\n",
      "Epoch 168, Loss: 0.0402, Acc: 0.9929\n",
      "Epoch 169, Loss: 0.0566, Acc: 0.9929\n",
      "Epoch 170, Loss: 0.0361, Acc: 0.9929\n",
      "Epoch 171, Loss: 0.0434, Acc: 1.0000\n",
      "Epoch 172, Loss: 0.0400, Acc: 0.9929\n",
      "Epoch 173, Loss: 0.0381, Acc: 0.9929\n",
      "Epoch 174, Loss: 0.0695, Acc: 0.9857\n",
      "Epoch 175, Loss: 0.0535, Acc: 0.9929\n",
      "Epoch 176, Loss: 0.0451, Acc: 0.9857\n",
      "Epoch 177, Loss: 0.0426, Acc: 0.9929\n",
      "Epoch 178, Loss: 0.0452, Acc: 0.9929\n",
      "Epoch 179, Loss: 0.0564, Acc: 0.9786\n",
      "Epoch 180, Loss: 0.0367, Acc: 0.9929\n",
      "Epoch 181, Loss: 0.0372, Acc: 1.0000\n",
      "Epoch 182, Loss: 0.0358, Acc: 1.0000\n",
      "Epoch 183, Loss: 0.0517, Acc: 1.0000\n",
      "Epoch 184, Loss: 0.0629, Acc: 0.9857\n",
      "Epoch 185, Loss: 0.0417, Acc: 1.0000\n",
      "Epoch 186, Loss: 0.0653, Acc: 0.9857\n",
      "Epoch 187, Loss: 0.0352, Acc: 0.9929\n",
      "Epoch 188, Loss: 0.0370, Acc: 1.0000\n",
      "Epoch 189, Loss: 0.0387, Acc: 0.9857\n",
      "Epoch 190, Loss: 0.0269, Acc: 1.0000\n",
      "Epoch 191, Loss: 0.0274, Acc: 1.0000\n",
      "Epoch 192, Loss: 0.0563, Acc: 0.9929\n",
      "Epoch 193, Loss: 0.0333, Acc: 1.0000\n",
      "Epoch 194, Loss: 0.0533, Acc: 0.9929\n",
      "Epoch 195, Loss: 0.0320, Acc: 1.0000\n",
      "Epoch 196, Loss: 0.0308, Acc: 1.0000\n",
      "Epoch 197, Loss: 0.0340, Acc: 1.0000\n",
      "Epoch 198, Loss: 0.0435, Acc: 0.9857\n",
      "Epoch 199, Loss: 0.0426, Acc: 0.9857\n",
      "Epoch 200, Loss: 0.0287, Acc: 1.0000\n",
      "Saved model to pth/gcn_epoch200.pth\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ========= 初始化模型 ==========\n",
    "model = GCN(nfeat=features.shape[1],                   # 输入维度是1433，也就是向量长度\n",
    "            nhid=16,\n",
    "            nclass=labels.max().item() + 1,\n",
    "            dropout=0.5)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)  # L2 正则化，权重衰减\n",
    "\n",
    "# ========= 创建模型保存目录 ==========\n",
    "os.makedirs(\"pth\", exist_ok=True)\n",
    "\n",
    "# ========= 开始训练 ==========\n",
    "for epoch in range(200):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    output = model(features, adj)  # 前向传播\n",
    "    loss = F.nll_loss(output[idx_train], labels[idx_train])  # 损失函数, 负对数似然损失(Negative Log Likelihood Loss，简称 NLL Loss)\n",
    "    acc = accuracy(output[idx_train], labels[idx_train])     # 训练准确率\n",
    "\n",
    "    loss.backward()    # 反向传播\n",
    "    optimizer.step()   # 参数更新\n",
    "\n",
    "    print(f\"Epoch {epoch+1:03d}, Loss: {loss.item():.4f}, Acc: {acc:.4f}\")\n",
    "\n",
    "    # ========= 每 50 次保存一次模型 ==========\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        model_path = f\"pth/gcn_epoch{epoch+1}.pth\"\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print(f\"Saved model to {model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "577b8c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.5173, Test Accuracy: 0.8400\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "output = model(features, adj)\n",
    "loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "print(f\"Test Loss: {loss_test.item():.4f}, Test Accuracy: {acc_test:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
