{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfe7036d",
   "metadata": {},
   "source": [
    "学习链接：https://zhuanlan.zhihu.com/p/89503068"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19e6a2a",
   "metadata": {},
   "source": [
    "## 图嵌入（Graph Embedding）\n",
    "\n",
    "图嵌入是一种 **表示学习（Representation Learning）** 技术，也称为 **网络嵌入**、**图表示学习** 或 **网络表示学习**。其目标是将图结构中的信息映射为低维空间中的向量表示，以便于后续的机器学习任务。\n",
    "\n",
    "### 1. 目标\n",
    "\n",
    "图嵌入主要包括两种类型：\n",
    "\n",
    "节点嵌入（Node Embedding）：将图中的每个节点表示成低维、实值、稠密的向量。\n",
    "\n",
    "*图级嵌入（Graph-level Embedding）：将整个图表示成一个低维、实值、稠密的向量。\n",
    "\n",
    "### 2. 图嵌入的方法\n",
    "\n",
    "图嵌入的实现方法多种多样，主要包括以下几类：\n",
    "\n",
    "#### 2.1 矩阵分解（Matrix Factorization）\n",
    "\n",
    "该方法通过构建和分解反映图结构的矩阵，获得节点或图的向量表示。常用的矩阵包括：\n",
    "\n",
    "* 邻接矩阵（Adjacency Matrix）\n",
    "* 拉普拉斯矩阵（Laplacian Matrix）\n",
    "* 节点转移概率矩阵（Transition Probability Matrix）\n",
    "* 节点属性矩阵（Node Feature Matrix）\n",
    "\n",
    "不同的矩阵类型适用于不同的分解策略，例如特征值分解、奇异值分解（SVD）等。此类方法适合结构静态、规模较小的图。\n",
    "\n",
    "#### 2.2 DeepWalk\n",
    "\n",
    "DeepWalk 是一种基于随机游走和 Word2Vec 模型的图嵌入方法。其核心思想包括：\n",
    "\n",
    "* 利用随机游走在图中生成节点序列；\n",
    "* 将这些序列视为“句子”，将节点视为“单词”；\n",
    "* 使用 Word2Vec 对这些“句子”进行训练，获得节点的嵌入表示。\n",
    "\n",
    "#### 2.3 图神经网络（Graph Neural Network, GNN）\n",
    "\n",
    "图神经网络通过结合图结构与节点特征，并使用神经网络进行端到端训练，实现图嵌入学习。GNN 类方法可以同时处理节点分类、图分类、链接预测等多种任务。\n",
    "\n",
    "**常见的 GNN 模型包括**：\n",
    "\n",
    "* GCN（Graph Convolutional Network）\n",
    "* GAT（Graph Attention Network）\n",
    "* GraphSAGE\n",
    "* Graph Isomorphism Network (GIN)\n",
    "\n",
    "GNN 不仅可以生成节点嵌入，也可以通过全局聚合机制生成图级嵌入，是目前图嵌入研究的核心方向之一。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79fab9b",
   "metadata": {},
   "source": [
    "## 3. 图嵌入方法图谱（Method Taxonomy）\n",
    "\n",
    "图嵌入方法可以从图类型、训练方法和传播方式（Propagating Step）等多个维度进行分类。如下是主要的分类图谱结构：\n",
    "\n",
    "### 3.1 按图类型分类（Graph Types）\n",
    "\n",
    "* **静态图（Static Graph）**\n",
    "* **异构图（Heterogeneous Graph）**\n",
    "\n",
    "  * Graph Inspection\n",
    "  * HAN\n",
    "* **边属性图（Edge-attributed Graph）**\n",
    "\n",
    "  * G2S\n",
    "  * R-GCN\n",
    "* **动态图（Dynamic Graph）**\n",
    "\n",
    "  * DCRNN\n",
    "  * STGCN\n",
    "  * Structural-RNN\n",
    "  * ST-GCN\n",
    "* **分级图（Graded Graph）**\n",
    "\n",
    "  * DGP\n",
    "\n",
    "### 3.2 按训练方法分类（Training Methods）\n",
    "\n",
    "* **邻居采样（Neighborhood Sampling）**\n",
    "\n",
    "  * GraphSAGE\n",
    "  * FastGCN\n",
    "  * PinSage\n",
    "  * SSE\n",
    "  * Adaptive Sampling\n",
    "* **感受野控制（Receptive Field Control）**\n",
    "\n",
    "  * Control Variate\n",
    "* **数据增强（Data Augmentation）**\n",
    "\n",
    "  * Co-training\n",
    "  * Self-training\n",
    "* **无监督训练（Unsupervised Training）**\n",
    "\n",
    "  * GAE\n",
    "  * VGAE\n",
    "  * ARGA\n",
    "  * GCMC\n",
    "\n",
    "### 3.3 按传播步骤分类（Propagation Step）\n",
    "\n",
    "* **卷积式聚合器（Convolutional Aggregator）**\n",
    "\n",
    "  * Graph Convolutional Networks（GCN）\n",
    "\n",
    "    * Spectral 类方法：Spectral Network, ChebNet, GCN\n",
    "    * Spatial 类方法：DCNN, MoNet, GraphSAGE 等\n",
    "* **注意力式聚合器（Attention Aggregator）**\n",
    "\n",
    "  * Graph Attention Network（GAT）\n",
    "  * Gated Attention Network\n",
    "* **门控更新器（Gate Updater）**\n",
    "\n",
    "  * GRU\n",
    "  * LSTM\n",
    "* **跳跃连接（Skip Connection）**\n",
    "\n",
    "  * Jump Knowledge Network\n",
    "  * Highway GNN\n",
    "* **分层结构（Hierarchical GNN）**\n",
    "\n",
    "  * ECC\n",
    "  * DIFFPOOL\n",
    "* **图结构 LSTM（Graph LSTM）**\n",
    "\n",
    "  * Tree LSTM\n",
    "  * Graph LSTM\n",
    "  * Sentence LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cfdcfe",
   "metadata": {},
   "source": [
    "## 4.图\n",
    "\n",
    "### 在图神经网络和图嵌入中基本定义：\n",
    "\n",
    "一个图记作：\n",
    "\n",
    "$$\n",
    "G = (V, E)\n",
    "$$\n",
    "\n",
    "其中：\n",
    "\n",
    "* $V$：节点的集合（Vertices）\n",
    "* $E$：边的集合（Edges），表示节点之间的连接关系\n",
    "\n",
    "### 节点特征表示：\n",
    "\n",
    "对于图中的每个节点 $i$，我们假设它拥有一个特征向量 $x_i$。将所有节点的特征组合在一起，可以构成一个特征矩阵：\n",
    "\n",
    "$$\n",
    "X \\in \\mathbb{R}^{N \\times D}\n",
    "$$\n",
    "\n",
    "其中：\n",
    "\n",
    "* $N$：节点数量（即 $|V|$）\n",
    "* $D$：每个节点的特征维度（即每个 $x_i \\in \\mathbb{R}^D$）\n",
    "\n",
    "这个矩阵 $X$ 就是图中所有节点的特征表示。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e6ecef",
   "metadata": {},
   "source": [
    "##  图的三种基本矩阵解释\n",
    "\n",
    "### 1. 邻接矩阵（Adjacency Matrix, $A$）\n",
    "\n",
    "邻接矩阵用来表示节点之间是否有**边相连**。\n",
    "\n",
    "* 是一个 $N \\times N$ 的矩阵（$N$ 是节点数）\n",
    "* 如果节点 $i$ 和节点 $j$ 有边相连，$A_{ij} = 1$，否则为 0。\n",
    "* 对于**无向图**，邻接矩阵是对称的。\n",
    "\n",
    "在图中可以看到：\n",
    "\n",
    "```\n",
    "    A =\n",
    "    0 1 1 0 0 0\n",
    "    1 0 1 1 0 0\n",
    "    1 1 0 0 1 0\n",
    "    0 1 0 0 1 1\n",
    "    0 0 1 1 0 0\n",
    "    0 0 0 1 0 0\n",
    "```\n",
    "\n",
    "每一行/列表示一个节点，相邻节点之间的值为 1。\n",
    "\n",
    "---\n",
    "\n",
    "### 2. 度矩阵（Degree Matrix, $D$）\n",
    "\n",
    "度矩阵是一个对角矩阵，用来表示每个节点的“度”——也就是它连接了几个边（邻居节点数）。\n",
    "\n",
    "* 是一个 $N \\times N$ 的**对角矩阵**\n",
    "* 第 $i$ 行第 $i$ 列的值 $D_{ii}$ 表示节点 $i$ 的度（它有多少邻居）\n",
    "\n",
    "在图中可以看到：\n",
    "\n",
    "```\n",
    "    D =\n",
    "    2 0 0 0 0 0\n",
    "    0 3 0 0 0 0\n",
    "    0 0 3 0 0 0\n",
    "    0 0 0 3 0 0\n",
    "    0 0 0 0 2 0\n",
    "    0 0 0 0 0 1\n",
    "```\n",
    "\n",
    "例如节点 2 有 3 个邻居，所以它的度为 3。\n",
    "\n",
    "---\n",
    "\n",
    "### 3. 拉普拉斯矩阵（Laplacian Matrix, $L$）\n",
    "\n",
    "拉普拉斯矩阵是最常用于图信号处理和图神经网络中的结构，它定义为：\n",
    "\n",
    "$$\n",
    "L = D - A\n",
    "$$\n",
    "\n",
    "即：**度矩阵 - 邻接矩阵**\n",
    "\n",
    "* 它综合反映了图的结构和连接方式\n",
    "* 常用于图卷积、频谱分析等领域\n",
    "\n",
    "在图中可以看到：\n",
    "\n",
    "```\n",
    "    L =\n",
    "    2 -1 -1  0  0  0\n",
    "   -1  3 -1 -1  0  0\n",
    "   -1 -1  3  0 -1  0\n",
    "    0 -1  0  3 -1 -1\n",
    "    0  0 -1 -1  2  0\n",
    "    0  0  0 -1  0  1\n",
    "```\n",
    "\n",
    "对角线元素是度，非对角线是邻接关系的负值。\n",
    "\n",
    "---\n",
    "\n",
    "## 总结\n",
    "\n",
    "| 矩阵                 | 表示意义          |\n",
    "| ------------------ | ------------- |\n",
    "| 邻接矩阵 $A$           | 哪些节点之间有边      |\n",
    "| 度矩阵 $D$            | 每个节点的连接数（度）   |\n",
    "| 拉普拉斯矩阵 $L = D - A$ | 图的结构，用于图卷积等操作 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c33c31d",
   "metadata": {},
   "source": [
    "## 5. 图卷积的公式理解\n",
    "\n",
    "GCN的核心在于如何基于图结构 进行特征的聚合与传播。\n",
    "\n",
    "### 总体框架：图卷积的一般形式\n",
    "\n",
    "$$\n",
    "H^{(l+1)} = f(H^{(l)}, A)\n",
    "$$\n",
    "\n",
    "常见的简化版本为：\n",
    "\n",
    "$$\n",
    "H^{(l+1)} = \\sigma(AH^{(l)}W^{(l)})\n",
    "$$\n",
    "\n",
    "其中：\n",
    "\n",
    "* $H^{(l)}$：第 $l$ 层节点的特征表示\n",
    "* $A$：邻接矩阵\n",
    "* $W^{(l)}$：第 $l$ 层的可训练权重矩阵\n",
    "* $\\sigma$：非线性激活函数（如 ReLU）\n",
    "\n",
    "\n",
    "### 实现一：基础图卷积（未归一化）\n",
    "\n",
    "$$\n",
    "H^{(l+1)} = \\sigma(AH^{(l)}W^{(l)})\n",
    "$$\n",
    "\n",
    "**特点：**\n",
    "\n",
    "* 聚合邻居特征，但没有考虑自身特征；\n",
    "* 邻接矩阵 $A$ 未归一化，容易造成高阶节点影响过大。\n",
    "\n",
    "类比：只听邻居的，不考虑自己的信息，且说话声音大小没有控制。\n",
    "\n",
    "\n",
    "### 实现二：引入拉普拉斯矩阵（考虑自身）\n",
    "\n",
    "$$\n",
    "H^{(l+1)} = \\sigma((D - A)H^{(l)}W^{(l)}) = \\sigma(L H^{(l)}W^{(l)})\n",
    "$$\n",
    "\n",
    "**特点：**\n",
    "\n",
    "* 使用组合拉普拉斯矩阵 $L = D - A$，引入自身节点的影响；\n",
    "* 缓解了特征传递的偏差问题。\n",
    "\n",
    "类比：开始考虑自己的声音，但邻居声音仍未做“降噪”处理。\n",
    "\n",
    "\n",
    "### 实现三：标准 GCN（对称归一化）\n",
    "\n",
    "$$\n",
    "H^{(l+1)} = \\sigma(\\hat{D}^{-1/2} \\hat{A} \\hat{D}^{-1/2} H^{(l)} W^{(l)})\n",
    "$$\n",
    "\n",
    "其中：\n",
    "\n",
    "* $\\hat{A} = A + I$：邻接矩阵加上自环\n",
    "* $\\hat{D}$：对应的度矩阵\n",
    "\n",
    "\n",
    "$\\hat{D}^{-1/2}$ 例子：\n",
    "\n",
    "$$\n",
    "d_1 = 3, \\quad d_2 = 2, \\quad d_3 = 1\n",
    "$$\n",
    "\n",
    "那么度矩阵：\n",
    "\n",
    "$$\n",
    "\\hat{D} =\n",
    "\\begin{bmatrix}\n",
    "3 & 0 & 0 \\\\\n",
    "0 & 2 & 0 \\\\\n",
    "0 & 0 & 1 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "则：\n",
    "\n",
    "$$\n",
    "\\hat{D}^{-1/2} =\n",
    "\\begin{bmatrix}\n",
    "\\frac{1}{\\sqrt{3}} & 0 & 0 \\\\\n",
    "0 & \\frac{1}{\\sqrt{2}} & 0 \\\\\n",
    "0 & 0 & 1 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**特点：**\n",
    "\n",
    "* 自身信息 + 邻居信息；\n",
    "* 一次性更新所有节点\n",
    "* 邻接矩阵归一化，控制不同度节点的权重影响；\n",
    "* 两边乘 可以使结果对称且保持节点间影响平衡\n",
    "* 是 **Kipf & Welling (2017)** 提出的主流 GCN 实现方式。\n",
    "\n",
    "类比：你和邻居都发声，而且大家的音量都控制在相似水平，不让“话痨”主导讨论。\n",
    "\n",
    "### 从节点角度看更新公式（局部视角）\n",
    "\n",
    "$$\n",
    "h_i^{(l+1)} = \\sigma\\left(\\sum_{j \\in \\mathcal{N}(i) \\cup \\{i\\}} \\frac{1}{\\sqrt{d_i d_j}} h_j^{(l)} W^{(l)}\\right)\n",
    "$$\n",
    "\n",
    "\n",
    "where：\n",
    "* $h_j^{(l)}$：是**节点 $j$** 在第 $l$ 层的特征向量\n",
    "* $d_i$：是节点 $i$ 的度（连接多少节点，**包含自己**，因为 $\\hat{A} = A + I$）\n",
    "* $\\frac{1}{\\sqrt{d_i d_j}}$：归一化因子，避免高/低度节点影响失衡。\n",
    "\n",
    "\n",
    "### 实现方式对比总结：\n",
    "\n",
    "| 实现编号 | 公式形式                                               | 优点         | 缺点         |\n",
    "| ---- | -------------------------------------------------- | ---------- | ---------- |\n",
    "| 实现一  | $\\sigma(AHW)$                                      | 简洁直接       | 忽略自身，未归一化  |\n",
    "| 实现二  | $\\sigma(LHW)$                                      | 引入自环       | 无归一化，表达力弱  |\n",
    "| 实现三  | $\\sigma(\\hat{D}^{-1/2} \\hat{A} \\hat{D}^{-1/2} HW)$ | 平衡归一化、考虑自身 | 主流方案，结构更复杂 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c735f9b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
